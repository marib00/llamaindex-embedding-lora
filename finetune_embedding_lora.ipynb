{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44986dbb",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/marib00/llamaindex-embedding-lora/finetune_embedding_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5ac7e-d36d-4879-959a-1af414fe4c02",
   "metadata": {},
   "source": [
    "# LoRA finetuning of any Black-Box Embedding Model\n",
    "\n",
    "This notebook is based on https://github.com/run-llama/llama_index/blob/3e5d0a146fcda01a984818d381f31a19287aead8/docs/examples/finetuning/embeddings/finetune_embedding_adapter.ipynb and demonstrates how to:\n",
    "\n",
    "- Generate a fine-tuning corpus using a local LLM\n",
    "- Fine-tune a local embedding model using LoRA\n",
    "\n",
    "The latter is achieved by subclassing the `EmbeddingAdapterFinetuneEngine` and a few tricks in order to make it behave (in the way we want it to)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6c5cc-8b31-41cd-95aa-6d60fbefff9b",
   "metadata": {},
   "source": [
    "## Generate Corpus\n",
    "\n",
    "We use our helper abstractions, `generate_qa_embedding_pairs`, to generate our training and evaluation dataset. This function takes in any set of text nodes (chunks) and generates a structured dataset containing (question, context) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b36f73f-83b1-4715-bd4d-7ce1353d1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Any, List, Optional, Tuple#, Union\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.base.embeddings.base import BaseEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface.base import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.huggingface.pooling import Pooling\n",
    "from llama_index.finetuning import EmbeddingAdapterFinetuneEngine\n",
    "from llama_index.finetuning.embeddings.adapter_utils import BaseAdapter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fc4bd24",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae97522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-18 14:51:34--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1880483 (1.8M) [application/octet-stream]\n",
      "Saving to: ‘data/10k/uber_2021.pdf’\n",
      "\n",
      "data/10k/uber_2021. 100%[===================>]   1.79M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-03-18 14:51:34 (41.6 MB/s) - ‘data/10k/uber_2021.pdf’ saved [1880483/1880483]\n",
      "\n",
      "--2024-03-18 14:51:34--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1440303 (1.4M) [application/octet-stream]\n",
      "Saving to: ‘data/10k/lyft_2021.pdf’\n",
      "\n",
      "data/10k/lyft_2021. 100%[===================>]   1.37M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2024-03-18 14:51:35 (43.5 MB/s) - ‘data/10k/lyft_2021.pdf’ saved [1440303/1440303]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/10k/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c43042-2ed1-4ab7-a53d-7f65dd856f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES = [\"./data/10k/lyft_2021.pdf\"]\n",
    "VAL_FILES = [\"./data/10k/uber_2021.pdf\"]\n",
    "\n",
    "TRAIN_CORPUS_FPATH = \"./data/train_corpus.json\"\n",
    "VAL_CORPUS_FPATH = \"./data/val_corpus.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7e38d0-39ff-44e2-ab7f-fded56dcd707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(files, verbose=False):\n",
    "    if verbose: print(f\"Loading files {files}\")\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_files=files)\n",
    "    docs = reader.load_data()\n",
    "    if verbose: print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "    parser = SentenceSplitter()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "    if verbose: print(f\"Parsed {len(nodes)} nodes\")\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1257dce-0be1-42c4-9346-a1fe68505fdd",
   "metadata": {},
   "source": [
    "We do a very naive train/val split by having the Lyft corpus as the train dataset, and the Uber corpus as the val dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd6d8af-5382-48b8-8a7d-98a03d2f150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files ['./data/10k/lyft_2021.pdf']\n",
      "Loaded 238 docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e3746a01034f1387e563606519788d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 344 nodes\n",
      "Loading files ['./data/10k/uber_2021.pdf']\n",
      "Loaded 307 docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3980765635d540168c0c12f90f4cba92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 410 nodes\n"
     ]
    }
   ],
   "source": [
    "train_nodes = load_corpus(TRAIN_FILES, verbose=True)\n",
    "val_nodes = load_corpus(VAL_FILES, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893a5f1-6fdf-473b-80ea-5ea3df5681a7",
   "metadata": {},
   "source": [
    "### Generate synthetic queries\n",
    "\n",
    "Now, we use an LLM (Mixtral) to generate questions using each text chunk in the corpus as context.\n",
    "\n",
    "Each pair of (generated question, text chunk used as context) becomes a datapoint in the finetuning dataset (either for training or evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1c892e-e27d-49f6-96d4-b99af330aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9eddecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama-index/lib/python3.11/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "model_id = 'TheBloke/Mixtral-8x7B-v0.1-GPTQ'\n",
    "code_revision = 'gptq-4bit-32g-actorder_True'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, attn_implementation='flash_attention_2')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, code_revision=code_revision, device_map='auto')\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    query_wrapper_prompt=PromptTemplate('[INST] {query_str} [/INST]'),\n",
    "    context_window=16*1024,\n",
    "    max_new_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330fb1f-cfb4-4b9b-b614-06910d5330b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_qa_embedding_pairs(train_nodes, llm=llm)\n",
    "train_dataset.save_json(\"train_dataset.json\")\n",
    "\n",
    "val_dataset = generate_qa_embedding_pairs(val_nodes, llm=llm)\n",
    "val_dataset.save_json(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909ca757-bf02-4304-a59e-7d61a12a67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release cuda memory - at this point it's probably a good idea to restart the kernel and load the data\n",
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "\n",
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n",
    "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619e9a6-4795-4ff5-bb48-ae2c50324eb2",
   "metadata": {},
   "source": [
    "## Run Embedding Finetuning\n",
    "\n",
    "Here we first define the subclasses needed for LoRA finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea49d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalAdapter(torch.nn.Identity, BaseAdapter):\n",
    "    \"\"\"Adapter model that does nothing, but includes trainable parameters \n",
    "    (e.g. LoRAs) of the embedding model, which the FinetuneEngine actually trains.\"\"\"\n",
    "    def __init__(self, embed_model):\n",
    "        super().__init__()\n",
    "        self.embed_model = embed_model\n",
    "\n",
    "    def save(self, output_path):\n",
    "        self.embed_model.save_pretrained(output_path, save_adapter=True, save_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5234bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalEmbeddingFinetuneEngine(EmbeddingAdapterFinetuneEngine):\n",
    "    \"\"\"Fintune any parameters of embed_model with requires_grad set to True, e.g. LoRA adapaters.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: EmbeddingQAFinetuneDataset,\n",
    "        embed_model: BaseEmbedding,\n",
    "        batch_size: int = 10,\n",
    "        epochs: int = 1,\n",
    "        dim: Optional[int] = None,\n",
    "        device: Optional[str] = None,\n",
    "        model_output_path: str = \"model_output\",\n",
    "        model_checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_save_steps: int = 100,\n",
    "        verbose: bool = False,\n",
    "        bias: bool = False,\n",
    "        **train_kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            dataset=dataset,\n",
    "            embed_model=embed_model,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            adapter_model=UniversalAdapter(embed_model._model),\n",
    "            dim=dim,\n",
    "            device=device,\n",
    "            model_output_path=model_output_path,\n",
    "            model_checkpoint_path=model_checkpoint_path,\n",
    "            checkpoint_save_steps=checkpoint_save_steps,\n",
    "            verbose=verbose,\n",
    "            bias=bias,\n",
    "            **train_kwargs,\n",
    "        )\n",
    "\n",
    "    def smart_batching_collate(self, batch: List) -> Tuple[Any, Any]:\n",
    "        \"\"\"Smart batching collate.\"\"\"\n",
    "        import torch\n",
    "        from torch import Tensor\n",
    "\n",
    "        query_embeddings: List[Tensor] = []\n",
    "        text_embeddings: List[Tensor] = []\n",
    "\n",
    "        for query, text in batch:\n",
    "            query_embedding = self.embed_model.get_query_embedding(query)\n",
    "            text_embedding = self.embed_model.get_text_embedding(text)\n",
    "\n",
    "            query_embeddings.append(query_embedding)    # was stripping gradients: query_embeddings.append(torch.tensor(query_embedding))\n",
    "            text_embeddings.append(text_embedding)      # was stripping gradients: text_embeddings.append(torch.tensor(text_embedding))\n",
    "\n",
    "        query_embeddings_t = torch.stack(query_embeddings)\n",
    "        text_embeddings_t = torch.stack(text_embeddings)\n",
    "\n",
    "        return query_embeddings_t, text_embeddings_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc9837a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceEmbeddingWithGrad(HuggingFaceEmbedding):\n",
    "    \"\"\"HuggingFaceEmbedding with gradient support.\"\"\"\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        return getattr(self._model, name)\n",
    "    \n",
    "    def _embed(self, sentences: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Embed sentences.\"\"\"\n",
    "        encoded_input = self._tokenizer(\n",
    "            sentences,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # pop token_type_ids\n",
    "        encoded_input.pop(\"token_type_ids\", None)\n",
    "\n",
    "        # move tokenizer inputs to device\n",
    "        encoded_input = {\n",
    "            key: val.to(self._device) for key, val in encoded_input.items()\n",
    "        }\n",
    "\n",
    "        model_output = self._model(**encoded_input)\n",
    "\n",
    "        context_layer: \"torch.Tensor\" = model_output[0]\n",
    "        if self.pooling == Pooling.CLS:\n",
    "            embeddings = self.pooling.cls_pooling(context_layer)\n",
    "        elif self.pooling == Pooling.LAST:\n",
    "            embeddings = self.pooling.last_pooling(context_layer)           \n",
    "        else:\n",
    "            embeddings = self._mean_pooling(\n",
    "                token_embeddings=context_layer,\n",
    "                attention_mask=encoded_input[\"attention_mask\"],\n",
    "            )\n",
    "\n",
    "        if self.normalize:\n",
    "            import torch\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings  # was embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "837cb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import fields as pydantic_fields\n",
    "\n",
    "class disable_pydantic:\n",
    "    \"\"\"Context manager to disable pydantic validation.\"\"\"\n",
    "\n",
    "    def __enter__(self) -> None:\n",
    "        self.validate = pydantic_fields.ModelField.validate\n",
    "        pydantic_fields.ModelField.validate = lambda *args, **kwargs: (args[1], None)\n",
    "\n",
    "    def __exit__(self, *args) -> None:\n",
    "        pydantic_fields.ModelField.validate = self.validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb224e",
   "metadata": {},
   "source": [
    "### Fine-tune sfr-embedding-mistral\n",
    "\n",
    "As of March 2024 SFR-Embedding-Mistral is at the top of the Massive Text Embedding Benchmark (MTEB) Leaderboard: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef7b91",
   "metadata": {},
   "source": [
    "We quantize the model to 4-bit first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b9b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'Salesforce/SFR-Embedding-Mistral'\n",
    "quant_path = f'/tmp/models/{model_id.replace(\"/\",\"-\")}-quant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f42ec76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5717387749124b5e9248cbeee6c428f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to /tmp/models/Salesforce-SFR-Embedding-Mistral-quant\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# freeze the model before saving just as a precaution\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.save_pretrained(quant_path, low_cpu_mem_usage=False)\n",
    "print(f'Quantized model saved to {quant_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09683b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release cuda memory\n",
    "del model, tokenizer, bnb_config\n",
    "import gc; gc.collect()\n",
    "with torch.no_grad(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "811fbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_adapters_path = '/tmp/whatever'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15bed4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
    "embed_model = AutoModel.from_pretrained(quant_path, low_cpu_mem_usage=True)\n",
    "embed_model.to = lambda _: embed_model  # quantized model does not have .to() method\n",
    "for param in embed_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e437c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_base_model = HuggingFaceEmbedding(\n",
    "    model=embed_model, \n",
    "    tokenizer=embed_tokenizer, \n",
    "    query_instruction='Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:',\n",
    "    pooling='last',\n",
    "    embed_batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf592ed6",
   "metadata": {},
   "source": [
    "Evaluate the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3068379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e164db6a4bb74df0bb81c3818a022984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 861/861 [01:56<00:00,  7.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base_sfr</td>\n",
       "      <td>0.872242</td>\n",
       "      <td>0.68494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  retrievers  hit_rate      mrr\n",
       "0   base_sfr  0.872242  0.68494"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from eval_utils import evaluate, display_results\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_sfr_val_results = evaluate(val_dataset, hf_base_model)\n",
    "display_results([\"base_sfr\"], [base_sfr_val_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "605768ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the peft model\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"FEATURE_EXTRACTION\",\n",
    ")\n",
    "\n",
    "kbit_model = prepare_model_for_kbit_training(embed_model)\n",
    "peft_model = get_peft_model(kbit_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or  load trained adapters\n",
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(embed_model, lora_adapters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b31b0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_qlora_model = HuggingFaceEmbeddingWithGrad(\n",
    "    model=peft_model, \n",
    "    tokenizer=embed_tokenizer, \n",
    "    query_instruction='Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:',\n",
    "    pooling='last',\n",
    "    embed_batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f1bf68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fa371cb6a24eab86b86b8b15eb2838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ef9d7d89974bf5a03a7a3869d6fc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama-index/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/llama-index/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /tmp/models/Salesforce-SFR-Embedding-Mistral-quant - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "finetune_engine = UniversalEmbeddingFinetuneEngine(\n",
    "    train_dataset,\n",
    "    embed_model=hf_qlora_model,\n",
    "    dim=4096,\n",
    "    model_output_path=lora_adapters_path,\n",
    "    epochs=5,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "with disable_pydantic():\n",
    "    finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14dd5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repackage as HuggingFaceEmbedding to avoid grief from pydantic which wants embeddings to be lists not tensors\n",
    "hf_embeddig_model = HuggingFaceEmbedding(\n",
    "    model=hf_qlora_model.model, \n",
    "    tokenizer=hf_qlora_model._tokenizer, \n",
    "    query_instruction=hf_qlora_model.query_instruction,\n",
    "    pooling=hf_qlora_model.pooling,\n",
    "    embed_batch_size=hf_qlora_model.embed_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d25a60e",
   "metadata": {},
   "source": [
    "Evaluate the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bda7f46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63255805c566460bb3cd003ed029fc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama-index/lib/python3.11/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 861/861 [01:59<00:00,  7.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lora_sfr</td>\n",
       "      <td>0.941928</td>\n",
       "      <td>0.803949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  retrievers  hit_rate       mrr\n",
       "0   lora_sfr  0.941928  0.803949"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from eval_utils import evaluate, display_results\n",
    "\n",
    "with torch.no_grad():\n",
    "    lora_sfr_val_results = evaluate(val_dataset, hf_embeddig_model)\n",
    "display_results([\"lora_sfr\"], [lora_sfr_val_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5e9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
